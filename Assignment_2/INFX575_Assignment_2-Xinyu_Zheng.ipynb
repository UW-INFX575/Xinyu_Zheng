{
 "metadata": {
  "name": "",
  "signature": "sha256:1b252854545719f6f7292a6f4819577b365b8cea83282e3124e68578515249b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import libraries for processing\n",
      "import nltk\n",
      "import unicodedata\n",
      "import string\n",
      "from nltk import stem\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk import bigrams\n",
      "from nltk import trigrams\n",
      "\n",
      "# initialize stopword list, single letters, special characters to be removed from each text file\n",
      "stopset = set(stopwords.words('english'))\n",
      "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
      "special_char = ['~~']\n",
      "\n",
      "# initialize names of original text files and output files\n",
      "file_names = ['6334220.txt','6334221.txt','6334222.txt','6334223.txt','6334224.txt','6334225.txt','6334226.txt','6334227.txt','6334228.txt','6334229.txt']\n",
      "uni_files = ['6334220_uni.txt','6334221_uni.txt','6334222_uni.txt','6334223_uni.txt','6334224_uni.txt','6334225_uni.txt','6334226_uni.txt','6334227_uni.txt','6334228_uni.txt','6334229_uni.txt']\n",
      "bi_files = ['6334220_bi.txt','6334221_bi.txt','6334222_bi.txt','6334223_bi.txt','6334224_bi.txt','6334225_bi.txt','6334226_bi.txt','6334227_bi.txt','6334228_bi.txt','6334229_bi.txt']\n",
      "tri_files = ['6334220_tri.txt','6334221_tri.txt','6334222_tri.txt','6334223_tri.txt','6334224_tri.txt','6334225_tri.txt','6334226_tri.txt','6334227_tri.txt','6334228_tri.txt','6334229_tri.txt']\n",
      "\n",
      "# initialize global arrays to store output data to be saved to output files\n",
      "number_text = len(file_names)+1\n",
      "\n",
      "# arrays to store text before adding counts\n",
      "uni_cleaned = [[]]*number_text\n",
      "bi_cleaned = [[]]*number_text\n",
      "tri_cleaned = [[]]*number_text\n",
      "\n",
      "# arrays to store text with counts\n",
      "uni_count = [[]]*number_text\n",
      "bi_count = [[]]*number_text\n",
      "tri_count = [[]]*number_text\n",
      "\n",
      "# define a function to process each individual text file and output unigrams, bigrams, and trigrams\n",
      "def n_grams_individual(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        # split text file into tokens\n",
      "        tokens=word_tokenize(str(text))    \n",
      "        uni_tokens = [w for w in tokens if not w in stopset]\n",
      "        \n",
      "        # remove stop words, stem words, digits, single letters, and special characters\n",
      "        # and get cleaned unigrams\n",
      "        uni_stemmed = []\n",
      "        stemmer=stem.PorterStemmer()\n",
      "        for w in uni_tokens:\n",
      "            w = w.lower()\n",
      "            tmp = stemmer.stem(w)\n",
      "            tmp = unicodedata.normalize('NFKD', tmp).encode('ascii','ignore')\n",
      "            if tmp not in string.punctuation:\n",
      "                if not tmp.isdigit():\n",
      "                    if tmp not in letters:\n",
      "                        if tmp not in special_char:\n",
      "                            uni_stemmed.append(tmp)\n",
      "                            uni_cleaned[index] = uni_stemmed\n",
      "        \n",
      "        # generate cleaned bigrams from cleaned unigrams\n",
      "        bi_stemmed = []\n",
      "        bi_tokens = bigrams(uni_stemmed) \n",
      "        for w in bi_tokens:\n",
      "            bi_stemmed.append(w)\n",
      "            bi_cleaned[index] = bi_stemmed\n",
      "        \n",
      "        # generate cleaned trigrams from cleaned unigrams\n",
      "        tri_stemmed = []\n",
      "        tri_tokens = trigrams(uni_stemmed) \n",
      "        for w in tri_tokens:\n",
      "            tri_stemmed.append(w)\n",
      "            tri_cleaned[index] = tri_stemmed\n",
      "        \n",
      "        # generate cleaned unigrams with counts\n",
      "        uni_count[index] = [(item, uni_stemmed.count(item)) for item in uni_stemmed]\n",
      "        \n",
      "        # generate cleaned bigrams with counts\n",
      "        bi_stemmed_cleaned = []\n",
      "        for item in bi_stemmed:\n",
      "            item = (' '.join(str(s) for s in item))\n",
      "            bi_stemmed_cleaned.append(item)\n",
      "        bi_count[index] = [(item, bi_stemmed_cleaned.count(item)) for item in bi_stemmed_cleaned]\n",
      "        \n",
      "        # generate cleaned trigrams with counts\n",
      "        tri_stemmed_cleaned = []\n",
      "        for item in tri_stemmed:\n",
      "            item = (' '.join(str(s) for s in item))\n",
      "            tri_stemmed_cleaned.append(item)\n",
      "        tri_count[index] = [(item, tri_stemmed_cleaned.count(item)) for item in tri_stemmed_cleaned]\n",
      "\n",
      "    return True\n",
      "\n",
      "# process for each individual original text file\n",
      "for i in range(len(file_names)):\n",
      "    n_grams_individual(file_names[i], i)\n",
      "    \n",
      "    # write cleaned unigrams with counts to a uni_file\n",
      "    with open(uni_files[i], 'w') as f:\n",
      "        for uni in uni_count[i]:   \n",
      "            f.write(', '.join(str(s) for s in uni) + '\\n')\n",
      "        f.close()\n",
      "    \n",
      "    # write cleaned bigrams with counts to a bi_file\n",
      "    with open(bi_files[i], 'w') as f:\n",
      "        for bi in bi_count[i]:        \n",
      "            f.write(', '.join(str(s) for s in bi) + '\\n')\n",
      "        f.close()\n",
      "    \n",
      "    # write cleaned trigrams with counts to a tri_file\n",
      "    with open(tri_files[i], 'w') as f:\n",
      "        for tri in tri_count[i]:        \n",
      "            f.write(', '.join(str(s) for s in tri) + '\\n')\n",
      "        f.close()\n",
      "\n",
      "# define a function to combine 10 text files to 1 text file\n",
      "uni_tokens = []\n",
      "def n_grams_combined(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        tokens=word_tokenize(str(text)) \n",
      "        for w in tokens:\n",
      "            if not w in stopset:\n",
      "                uni_tokens.append(w)\n",
      "    return uni_tokens\n",
      "\n",
      "# combined all words in all 10 files\n",
      "for i in range(len(file_names)):\n",
      "    n_grams_combined(file_names[i], i)\n",
      "\n",
      "# write all words in all 10 files into 1 file  called 'combined.txt'\n",
      "with open('combined.txt', 'w') as f:\n",
      "    for uni in uni_tokens:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      "\n",
      "# process unigrams, bigrams, and trigrams for this combined file\n",
      "j = len(file_names)\n",
      "n_grams_individual('combined.txt', j)\n",
      "\n",
      "# write cleaned unigrams with counts to a uni_file\n",
      "with open('combined_uni.txt', 'w') as f:\n",
      "    for uni in uni_count[j]:   \n",
      "        f.write(', '.join(str(s) for s in uni) + '\\n')\n",
      "    f.close()\n",
      "\n",
      "# write cleaned bigrams with counts to a bi_file\n",
      "with open('combined_bi.txt', 'w') as f:\n",
      "    for bi in bi_count[j]:        \n",
      "        f.write(', '.join(str(s) for s in bi) + '\\n')\n",
      "    f.close()\n",
      "\n",
      "# write cleaned trigrams with counts to a tri_file\n",
      "with open('combined_tri.txt', 'w') as f:\n",
      "    for tri in tri_count[j]:        \n",
      "        f.write(', '.join(str(s) for s in tri) + '\\n')\n",
      "    f.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 154
    }
   ],
   "metadata": {}
  }
 ]
}