{
 "metadata": {
  "name": "",
  "signature": "sha256:f48ae8b916124d4b85ddc995a90fb0e7513eb3ee73d5982f6a154c7e4fc15a2e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import libraries for processing\n",
      "import nltk\n",
      "import unicodedata\n",
      "import string\n",
      "import math\n",
      "import time\n",
      "from nltk.tokenize import word_tokenize\n",
      "from collections import Counter\n",
      "from scipy.cluster.hierarchy import dendrogram\n",
      "from scipy.cluster.hierarchy import linkage\n",
      "from matplotlib.pyplot import show"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# split original abstracts file and remove blank docs\n",
      "def split_docs(a_file):\n",
      "    count_docs = 0\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        text = text.splitlines()        \n",
      "        for w in text:            \n",
      "            if (w[-4:] != 'null'):\n",
      "                new_abstracts.append(w)\n",
      "                count_docs = count_docs + 1       \n",
      "    return count_docs\n",
      "\n",
      "# assign docs into different groups\n",
      "def split_groups(abstracts):\n",
      "    for doc in abstracts:\n",
      "        doc = doc.split('\\t')\n",
      "        for i in range(0,10):            \n",
      "            if (groups[doc[0]] == str(i+1)):\n",
      "                groups_docs[i].append(doc[1])        \n",
      "    return True\n",
      "\n",
      "# clean words for each group - remove stopwords and punctuation\n",
      "def clean_words(a_group):\n",
      "    group_words = []\n",
      "    for d in a_group:\n",
      "        tmp = str(d)\n",
      "        tmp = tmp.decode('utf-8')\n",
      "        tokens=word_tokenize(tmp) \n",
      "        for w in tokens:\n",
      "            w = unicodedata.normalize('NFKD', w).encode('ascii','ignore') \n",
      "            if (w not in stopwords):\n",
      "                if w not in string.punctuation:\n",
      "                    group_words.append(w)\n",
      "    return group_words\n",
      "\n",
      "# calculate H value for each group\n",
      "def get_H(group_word):\n",
      "    log_sum_group = 0\n",
      "    prob_count = Counter(group_word)\n",
      "    freq_sum = sum(prob_count.itervalues())\n",
      "    vocab = list(set(group_word))\n",
      "    for w in vocab:  \n",
      "        w_prob = prob_count[w]/float(freq_sum)\n",
      "        if (w_prob > 0):\n",
      "            log_sum_group += -w_prob * math.log(w_prob,2)\n",
      "    return log_sum_group\n",
      "\n",
      "# calculate Q values between groups\n",
      "def get_Qij(groupi_word,groupj_word,groupij_word):\n",
      "    tmp_Qij = 0\n",
      "    alpha = 0.01\n",
      "    \n",
      "    count = Counter(groupi_word)\n",
      "    freq = sum(count.itervalues())\n",
      "    vocab = list(set(groupi_word))\n",
      "    \n",
      "    count_ij = Counter(groupij_word)\n",
      "    freq_ij = sum(count_ij.itervalues())\n",
      "    \n",
      "    count_j = Counter(groupj_word)\n",
      "    freq_j = sum(count_j.itervalues())\n",
      "    \n",
      "    for w in vocab:\n",
      "        pi = (1-alpha)*count[w]/float(freq)+alpha*count_ij[w]/float(freq_ij)\n",
      "        pj = (1-alpha)*count_j[w]/float(freq_j)+alpha*count_ij[w]/float(freq_ij)\n",
      "\n",
      "        tmp_Qij += pi * math.log(pj,2)\n",
      "    Q_ij = 0 - tmp_Qij\n",
      "    return Q_ij"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start_time = time.time() \n",
      "\n",
      "# initialize array to save result, file names, and new files to save result\n",
      "file_name = 'abstracts2.txt'\n",
      "stopwords = [\"all\",\"just\",\"being\",\"over\",\"both\",\"through\",\"yourselves\",\"its\",\"before\",\"herself\",\"had\",\"should\",\"to\",\"only\",\"under\",\"ours\",\"has\",\"do\",\"them\",\"his\",\"very\",\"they\",\"not\",\"during\",\"now\",\"him\",\"nor\",\"did\",\"this\",\"she\",\"each\",\"further\",\"where\",\"few\",\"because\",\"doing\",\"some\",\"are\",\"our\",\"ourselves\",\"out\",\"what\",\"for\",\"while\",\"does\",\"above\",\"between\",\"t\",\"be\",\"we\",\"who\",\"were\",\"here\",\"hers\",\"by\",\"on\",\"about\",\"of\",\"against\",\"s\",\"or\",\"own\",\"into\",\"yourself\",\"down\",\"your\",\"from\",\"her\",\"their\",\"there\",\"been\",\"whom\",\"too\",\"themselves\",\"was\",\"until\",\"more\",\"himself\",\"that\",\"but\",\"don\",\"with\",\"than\",\"those\",\"he\",\"me\",\"myself\",\"these\",\"up\",\"will\",\"below\",\"can\",\"theirs\",\"my\",\"and\",\"then\",\"is\",\"am\",\"it\",\"an\",\"as\",\"itself\",\"at\",\"have\",\"in\",\"any\",\"if\",\"again\",\"no\",\"when\",\"same\",\"how\",\"other\",\"which\",\"you\",\"after\",\"most\",\"such\",\"why\",\"a\",\"off\",\"i\",\"yours\",\"so\",\"the\",\"having\",\"once\"]\n",
      "new_abstracts = []\n",
      "\n",
      "# remove blank docs\n",
      "split_docs(file_name)\n",
      "\n",
      "# generate a dictionary of doc ids and group ids\n",
      "group = [line.strip() for line in open('groups2.txt')]\n",
      "group.pop(0)\n",
      "groups = {}\n",
      "for a_doc in group:\n",
      "    a_doc = a_doc.split('\\t')\n",
      "    groups[a_doc[0]] = a_doc[1]\n",
      "\n",
      "# groups_docs is used to store documents of each group\n",
      "groups_docs = [[]]*10\n",
      "for i in range(0,10):\n",
      "    groups_docs[i] = []\n",
      "\n",
      "# remove the title line of abstracts2.txt\n",
      "new_abstracts = [line.strip() for line in open('abstracts2.txt')]\n",
      "new_abstracts.pop(0)\n",
      "\n",
      "# assign docs into groups\n",
      "split_groups(new_abstracts)\n",
      "\n",
      "# groups_words is used to store all words of each group after cleaning\n",
      "groups_words = [[]]*10\n",
      "for j in range(0,10):\n",
      "    groups_words[j] = clean_words(groups_docs[j])\n",
      "    \n",
      "# generate corpus with 'cleaned' groups\n",
      "corpus_words = [[]]*10\n",
      "for i in range(0,9):\n",
      "    corpus_words[i] = [[]]*10\n",
      "    for j in range(i+1,10): \n",
      "        corpus_words[i][j] = []\n",
      "        for wi in groups_words[i]:\n",
      "            corpus_words[i][j].append(wi)\n",
      "        for wj in groups_words[j]:\n",
      "            corpus_words[i][j].append(wj)\n",
      "\n",
      "# calculate H value for each group\n",
      "H = [0]*10\n",
      "for i in range(0,10):\n",
      "    H[i] = get_H(groups_words[i])\n",
      "\n",
      "# calculate Q values between groups\n",
      "Q = [[]]*10\n",
      "for i in range(0,10):\n",
      "    Q[i] = [[]]*10\n",
      "\n",
      "for i in range(0,9):   \n",
      "    for j in range(i+1,10):\n",
      "        Q[i][j] = get_Qij(groups_words[i],groups_words[j],corpus_words[i][j])\n",
      "        Q[j][i] = get_Qij(groups_words[j],groups_words[i],corpus_words[i][j])\n",
      "\n",
      "# calculate E values between groups\n",
      "E = [[]]*10\n",
      "for i in range(0,10):\n",
      "    E[i] = [[]]*10\n",
      "    \n",
      "for i in range(0,9):   \n",
      "    for j in range(i+1,10):\n",
      "        E[i][j] = H[i] / Q[i][j]\n",
      "        E[j][i] = H[j] / Q[j][i]\n",
      "\n",
      "# calculate C and E_C (estimated C) values between groups\n",
      "C = [[]]*10\n",
      "E_C = [[]]*10\n",
      "for i in range(0,10):\n",
      "    C[i] = [[]]*10\n",
      "    E_C[i] = [[]]*10\n",
      "for i in range(0,9):   \n",
      "    for j in range(i+1,10):\n",
      "        C[i][j] = 1 - E[i][j]\n",
      "        C[j][i] = 1 - E[j][i]\n",
      "        E_C[i][j] = (C[i][j]+C[j][i])/2\n",
      "        E_C[j][i] = (C[i][j]+C[j][i])/2\n",
      "for i in range(0,10):\n",
      "    E_C[i][i] = 0\n",
      "    \n",
      "# calculate average C values for each group\n",
      "C_avg = [0]*10\n",
      "\n",
      "for i in range(0,10):\n",
      "    for j in range(0,10):\n",
      "        if (i != j):\n",
      "            C_avg[i] += C[i][j]\n",
      "    C_avg[i] = C_avg[i]/9\n",
      "\n",
      "# generate a dictionary of group ids and field names\n",
      "field = [line.strip() for line in open('Field_Names.txt')]\n",
      "fields = {}\n",
      "for a_field in field:\n",
      "    a_field = a_field.split('\\t')\n",
      "    a_field = a_field[0].split(',')\n",
      "    fields[a_field[0]] = a_field[1]\n",
      "\n",
      "# assign labels to different groups\n",
      "label = []\n",
      "for i in range(0,10):\n",
      "    label.append(fields[str(i+1)])\n",
      "    print fields[str(i+1)], C_avg[i]\n",
      "    \n",
      "print \"Time taken: \" + str(round(time.time() - start_time,5)) + \" seconds\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Ecology and Evolution 0.199487404714\n",
        "Molecular and Cell Biology 0.235242129863\n",
        "Economics 0.179115635352\n",
        "Sociology 0.176297611124\n",
        "Probability and Statistics 0.201858364487\n",
        "Organizational and marketing 0.185490251844\n",
        "Law 0.232891757122\n",
        "Anthropology 0.217861087073\n",
        "Political Science 0.206203975093\n",
        "Education 0.207097182603\n",
        "Time taken: 41.881 seconds\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Z = linkage(E_C)\n",
      "dendrogram(Z,labels = label)\n",
      "show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    }
   ],
   "metadata": {}
  }
 ]
}