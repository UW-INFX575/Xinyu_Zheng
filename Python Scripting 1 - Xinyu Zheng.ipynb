{
 "metadata": {
  "name": "",
  "signature": "sha256:b03fcc16ccde54a6de798190608f10dc68672833053d3df7882f39115a4c301c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
      "html = urllib.urlopen(url).read()\n",
      "soup = BeautifulSoup(html)\n",
      "\n",
      "# kill all script and style elements\n",
      "for script in soup([\"script\", \"style\"]):\n",
      "    script.extract()    # rip it out\n",
      "\n",
      "# get text\n",
      "text = soup.get_text()\n",
      "\n",
      "# break into lines and remove leading and trailing space on each\n",
      "lines = (line.strip() for line in text.splitlines())\n",
      "# break multi-headlines into a line each\n",
      "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
      "# drop blank lines\n",
      "text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
      "\n",
      "# print(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scripting for Clarkson University\n",
      "with open(\"clarkson.html\") as file:\n",
      "    for line in file:\n",
      "        name_clarkson = re.search('<a href=\"(.*).html\">(.*)</a><br />', line)\n",
      "        \n",
      "        if name_clarkson is not None:\n",
      "            print_name_clarkson = name_clarkson.group(0)\n",
      "            print_name_clarkson = print_name_clarkson.strip('<br />')\n",
      "            print_name_clarkson = print_name_clarkson.strip('<a href=\"(.*).html\">')\n",
      "            print_name_clarkson = print_name_clarkson.strip('</a>')\n",
      "#             print print_name_clarkson\n",
      "            names = re.search('>(.*)', print_name_clarkson)\n",
      "            print_names = names.group(0).strip('>')\n",
      "#             print print_names\n",
      "            \n",
      "            first_names = re.search('(.*) ', print_names)\n",
      "#             print \"First Name: \", first_names.group(0)\n",
      "            \n",
      "            last_names = re.search(' (.*)', print_names)\n",
      "#             print \"Last Name: \", last_names.group(0)\n",
      "#             print \n",
      "            \n",
      "#     for line in file:\n",
      "#         title_clarkson = re.search('</a>(.*)<br />', line)\n",
      "#         if title_clarkson is not None:\n",
      "#             print_title_clarkson = title_clarkson.group(0)    \n",
      "#             title_instructor = re.search('(.*)Instructor(.*)', print_title_clarkson)\n",
      "#             title_professor = re.search('(.*)Professor(.*)', print_title_clarkson)\n",
      "#             if (title_instructor is not None) or (title_professor is not None):\n",
      "#                 print print_title_clarkson\n",
      "#                 print \n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scripting for Lewis & Clark College\n",
      "with open(\"lewisclark.txt\") as file:\n",
      "    num_names = 0\n",
      "    names = []\n",
      "    num_depts = 0\n",
      "    departments = []\n",
      "    \n",
      "    for line in file:\n",
      "        name_lewis = re.search('<a href=\"/live/profiles/(.*)</a>', line)\n",
      "        \n",
      "        if name_lewis is not None:\n",
      "            print_name_lewis = name_lewis.group(0)\n",
      "            print_name_lewis = print_name_lewis.strip('<a href=\"/live/profiles/')\n",
      "            print_name_lewis = print_name_lewis.strip('</a>')\n",
      "            print_name_lewis = re.search('>(.*)', print_name_lewis)\n",
      "            print_names = print_name_lewis.group(0).strip('>')\n",
      "            num_names = num_names + 1\n",
      "            names.append(print_names)\n",
      "            \n",
      "#             print print_names\n",
      "        department = re.search('<div class=\"lw_profiles_2\">(.*)</div>', line)   \n",
      "        \n",
      "        if department is not None:\n",
      "            print_dept = department.group(0)\n",
      "#             print print_dept\n",
      "            \n",
      "            print_dept = print_dept.strip('<div class=\"lw_profiles_2\">')\n",
      "            print_dept = print_dept.strip('</')\n",
      "            print_dept = re.search(' of(.*)', print_dept)\n",
      "            if print_dept is not None:\n",
      "                print_dept = print_dept.group(0).strip(' of ')\n",
      "                num_depts = num_depts + 1\n",
      "                departments.append(print_dept)\n",
      "#     print departments\n",
      "#     print num            \n",
      "\n",
      "    for each_dept in departments:\n",
      "\n",
      "        if ',' in each_dept:\n",
      "            each_dept = each_dept.split(',')\n",
      "            each_dept = each_dept[0]\n",
      "        if ';' in each_dept:\n",
      "            each_dept = each_dept.split(';')\n",
      "            each_dept = each_dept[0]\n",
      "            \n",
      "#     print names\n",
      "#     print num_names\n",
      "#     print\n",
      "#     print departments\n",
      "#     print num_depts\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# example code using Beautiful Soup\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep # be nice\n",
      "\n",
      "BASE_URL = \"http://www.chicagoreader.com\"\n",
      "\n",
      "def make_soup(url):\n",
      "    html = urlopen(url).read()\n",
      "    return BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "def get_category_links(section_url):\n",
      "    soup = make_soup(section_url)\n",
      "    boccat = soup.find(\"dl\", \"boccat\")\n",
      "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
      "    return category_links\n",
      "\n",
      "def get_category_winner(category_url):\n",
      "    soup = make_soup(category_url)\n",
      "    category = soup.find(\"h1\", \"headline\").string\n",
      "    winner = [h2.string for h2 in soup.findAll(\"h2\", \"boc1\")]\n",
      "    runners_up = [h2.string for h2 in soup.findAll(\"h2\", \"boc2\")]\n",
      "    return {\"category\": category,\n",
      "            \"category_url\": category_url,\n",
      "            \"winner\": winner,\n",
      "            \"runners_up\": runners_up}\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    food_n_drink = (\"http://www.chicagoreader.com/chicago/\"\n",
      "                    \"best-of-chicago-2011-food-drink/BestOf?oid=4106228\")\n",
      "    \n",
      "    categories = get_category_links(food_n_drink)\n",
      "\n",
      "    data = [] # a list to store our dictionaries\n",
      "    for x in range(0,15): # show the first 15\n",
      "        category = categories[x]\n",
      "        winner = get_category_winner(category)\n",
      "        data.append(winner)\n",
      "        #sleep(1) # be nice\n",
      "\n",
      "#     print data\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 221
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scripting Fordham University\n",
      "\n",
      "import re\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep # be nice\n",
      "import unicodedata\n",
      "import csv\n",
      "\n",
      "BASE_URL = \"http://www.business.fordham.edu/faculty/\"\n",
      "\n",
      "def make_soup(url):\n",
      "    html = urlopen(url).read()\n",
      "    return BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "faculties = []\n",
      "faculty_last_name = []\n",
      "faculty_first_name = []\n",
      "\n",
      "# find all faculty links\n",
      "def get_faculty_links(section_url):\n",
      "    soup = make_soup(section_url)   \n",
      "    links = soup.findAll(\"table\", {\"class\": \"alpha-list\"})\n",
      "    \n",
      "    for i in range(len(links)):\n",
      "        # get all faculty detailed links\n",
      "        for a in links[i].find_all('a', href=True):\n",
      "            faculty_url = BASE_URL + a['href']\n",
      "            if faculty_url != \"http://www.business.fordham.edu/faculty/#supernav\":\n",
      "                faculties.append(faculty_url) \n",
      "        # get all faculty names\n",
      "        for li in links[i].find_all('li'):\n",
      "            each_li = li\n",
      "            for strong in each_li.find_all('strong'):\n",
      "                name = strong.text\n",
      "                name = unicodedata.normalize('NFKD', name).encode('ascii','ignore')\n",
      "                name_tmp = name.split(',')\n",
      "                last_name = name_tmp[0]\n",
      "                first_name = name_tmp[1]\n",
      "                faculty_last_name.append(last_name)\n",
      "                faculty_first_name.append(first_name)\n",
      "    return faculties\n",
      "\n",
      "# get all faculty links\n",
      "faculty_info = get_faculty_links(\"http://www.business.fordham.edu/faculty/faculty.asp\")\n",
      "\n",
      "faculty_departments = []\n",
      "\n",
      "for faculty in range(len(faculties)):\n",
      "    faculty_departments.append(\"School of Business\")\n",
      "\n",
      "# def get_faculty_dept(faculty_url):\n",
      "#     soup = make_soup(faculty_url)\n",
      "#     dept = soup.find(\"div\", {\"class\": \"profiles\"})\n",
      "    \n",
      "#     tmp_p = 0\n",
      "#     for p in dept.find_all('p'):\n",
      "#         each_p = p       \n",
      "#         tmp_p = tmp_p + 1\n",
      "\n",
      "#         if (tmp_p == 2):\n",
      "#             br_find = each_p.find(\"br\")\n",
      "\n",
      "#             if (br_find is not None):\n",
      "#                 depart = each_p.text\n",
      "#                 break\n",
      "            \n",
      "#         elif (tmp_p == 3):\n",
      "#             depart = each_p.text\n",
      "#             break\n",
      "            \n",
      "#     return depart\n",
      "\n",
      "faculty_education = []\n",
      "\n",
      "def get_faculty_edu(faculty_url):\n",
      "    soup = make_soup(faculty_url)\n",
      "    education = soup.find(\"div\", {\"id\": \"edu\"})\n",
      "    education_return = None\n",
      "    for li in education.find_all('li'):\n",
      "        if ((re.search('PhD(.*)', li.text) is not None) or (re.search('Ph.D.(.*)', li.text) is not None) or (re.search('J.D.(.*)', li.text) is not None) or (re.search('JD(.*)', li.text) is not None) or (re.search('Juris Doctor(.*)', li.text) is not None)):\n",
      "            education_return = li.text\n",
      "            break\n",
      "    if (education_return is None):\n",
      "        for li_tmp in education.find_all('li'):\n",
      "            if ((re.search('Masters(.*)', li_tmp.text) is not None) or (re.search('MS(.*)', li_tmp.text) is not None) or (re.search('MPA(.*)', li_tmp.text) is not None) or (re.search('MA(.*)', li_tmp.text) is not None) or (re.search('MBA(.*)', li_tmp.text) is not None)):   \n",
      "                education_return = li_tmp.text\n",
      "                break\n",
      "    return education_return \n",
      "\n",
      "# print get_faculty_edu('http://business.fordham.edu/faculty/cole/index.asp')\n",
      "\n",
      "for url in faculty_info:\n",
      "#     goal_dept = get_faculty_dept(url)\n",
      "#     goal_dept = unicodedata.normalize('NFKD', goal_dept).encode('ascii','ignore')\n",
      "#     faculty_departments.append(goal_dept)\n",
      "    \n",
      "    goal_edu = get_faculty_edu(url)\n",
      "    if (goal_edu is not None):\n",
      "        goal_edu = unicodedata.normalize('NFKD', goal_edu).encode('ascii','ignore')\n",
      "#         goal_edu = goal_edu.strip('PhD:')\n",
      "#         goal_edu = goal_edu.strip('PhD,')\n",
      "#         goal_edu = goal_edu.strip('PhD in')\n",
      "#         goal_edu = goal_edu.strip('Ph.D.:')\n",
      "#         goal_edu = goal_edu.strip('Masters:')\n",
      "#         goal_edu = goal_edu.strip('MS,')\n",
      "#         goal_edu = goal_edu.strip('J.D.:')\n",
      "#         goal_edu = goal_edu.strip('JD:')\n",
      "#         goal_edu = goal_edu.strip('MPA,')\n",
      "#         goal_edu = goal_edu.strip('MA,')\n",
      "#         goal_edu = goal_edu.strip('Ed.D.,')\n",
      "#         goal_edu = goal_edu.strip('MBA,')\n",
      "#         goal_edu = goal_edu.strip('Masters in')\n",
      "        \n",
      "        \n",
      "    else: \n",
      "        goal_edu = 'N/A'\n",
      "    faculty_education.append(goal_edu)    \n",
      "\n",
      "faculty_all_info = []\n",
      "\n",
      "faculty_university = []\n",
      "for faculty in range(len(faculties)):\n",
      "    faculty_university.append(\"Fordham University\")\n",
      "\n",
      "faculty_all_info.append(faculty_first_name)\n",
      "faculty_all_info.append(faculty_last_name)\n",
      "faculty_all_info.append(faculty_university)\n",
      "faculty_all_info.append(faculty_departments)\n",
      "faculty_all_info.append(faculty_education)\n",
      "\n",
      "\n",
      "with open(\"output.csv\", \"wb\") as f:\n",
      "    writer = csv.writer(f)\n",
      "#     writer.writerows(a)    \n",
      "    for row in faculty_all_info:\n",
      "        writer.writerow(row)\n",
      "        \n",
      "        \n",
      "# for faculty in range(len(faculty_info)):\n",
      "#     print {\"First Name\": faculty_first_name[faculty],\n",
      "#         \"Last Name\": faculty_last_name[faculty],\n",
      "#         \"Department\": faculty_departments[faculty],\n",
      "#         \"Education\": faculty_education[faculty]}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 220
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scripting Lehigh University\n",
      "\n",
      "import re\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep # be nice\n",
      "import unicodedata\n",
      "import csv\n",
      "\n",
      "BASE_URL = \"http://www.lehigh.edu/engineering/faculty/\"\n",
      "\n",
      "def make_soup(url):\n",
      "    html = urlopen(url).read()\n",
      "    return BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "faculties = []\n",
      "faculty_last_name = []\n",
      "faculty_first_name = []\n",
      "label = [0]*7\n",
      "department = [0]*7\n",
      "faculty_url = []\n",
      "faculty_name = []\n",
      "faculty_department = []\n",
      "faculty_university = []\n",
      "\n",
      "# find all faculty links\n",
      "def get_faculty_info(section_url):\n",
      "    soup = make_soup(section_url)   \n",
      "    links = soup.find(\"div\", {\"class\": \"accordion vertical\"})\n",
      "    all_a = links.find_all(\"div\")\n",
      "    \n",
      "    for i in range(7):\n",
      "        # department\n",
      "        label[i] = links.find(\"label\", {\"for\": \"checkbox-\"+str(i+1)})\n",
      "        department[i] = label[i].text\n",
      "        department[i] = unicodedata.normalize('NFKD', department[i]).encode('ascii','ignore')\n",
      "        department[i] = department[i].strip(' [+/-]')\n",
      "        \n",
      "        \n",
      "        for a in all_a[i].find_all('a', href=True):\n",
      "            # url\n",
      "            faculty_url.append(BASE_URL + a['href'])\n",
      "            # name\n",
      "            tmp = a.text\n",
      "            tmp = unicodedata.normalize('NFKD', tmp).encode('ascii','ignore')\n",
      "#             tmp = tmp.split(' ')\n",
      "#             tmp_first_name = tmp[0]\n",
      "#             tmp_last_name = tmp[1] \n",
      "            \n",
      "            faculty_name.append(tmp)\n",
      "\n",
      "            faculty_department.append(department[i])\n",
      "            faculty_university.append(\"Lehigh University\")\n",
      "        \n",
      "    return True \n",
      "\n",
      "# print get_faculty_info('http://www.lehigh.edu/engineering/faculty/index.html')\n",
      "\n",
      "# for n in range(len(faculty_name)):    \n",
      "#     print faculty_name[n], \"+\", faculty_department[n]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 305
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# scripting University of Montana\n",
      "\n",
      "import re\n",
      "import os\n",
      "from bs4 import BeautifulSoup\n",
      "from urllib2 import urlopen\n",
      "from time import sleep # be nice\n",
      "import unicodedata\n",
      "import csv\n",
      "\n",
      "BASE_URL = \"http://hs.umt.edu/hs/faculty-list/default.php\"\n",
      "BASE_PERSONAL_URL = \"http://hs.umt.edu/hs\"\n",
      "\n",
      "def make_soup(url):\n",
      "    html = urlopen(url).read()\n",
      "    return BeautifulSoup(html, \"lxml\")\n",
      "\n",
      "letter_section = []\n",
      "faculty_detail = []\n",
      "faculty_full_name = []\n",
      "faculty_department = []\n",
      "faculty_url = []\n",
      "tmp_name = [0]*2\n",
      "faculty_first_name = []\n",
      "faculty_last_name = []\n",
      "faculty_university = []\n",
      "faculty_education = []\n",
      "faculty = []\n",
      "\n",
      "# find all faculty links\n",
      "def get_letter_sections(section_url):\n",
      "    soup = make_soup(section_url)   \n",
      "    links = soup.find(\"div\", {\"class\": \"letters\"})\n",
      "               \n",
      "    for a in links.find_all('a', href=True):\n",
      "        # url\n",
      "        letter_section.append(BASE_URL + a['href'])\n",
      "        \n",
      "    return True\n",
      "\n",
      "# find all faculty info\n",
      "def get_faculty_info(section_url):\n",
      "    soup = make_soup(section_url)   \n",
      "    links = soup.find(\"tbody\")\n",
      "    faculty_info = links.find_all(\"tr\")\n",
      "               \n",
      "    for tr in faculty_info:\n",
      "        faculty_detail.append(tr)\n",
      "    \n",
      "    return True\n",
      "\n",
      "\n",
      "# find all faculty education\n",
      "def get_faculty_edu(section_url):\n",
      "    soup = make_soup(section_url)   \n",
      "    edu = soup.find(\"div\", {\"class\": \"span12 mdb_details\"})\n",
      "    p_text = 'N/A'\n",
      "    tmp_num = 0\n",
      "    for p in edu.find_all(\"p\"):\n",
      "#         print p.text\n",
      "        if (re.search('(.*)PhD(.*)', p.text)) or (re.search('(.*)Ph.D.(.*)', p.text)):\n",
      "            tmp_num = tmp_num + 1\n",
      "            p_text = p.text\n",
      "            p_text = unicodedata.normalize('NFKD', p_text).encode('ascii','ignore')\n",
      "            break\n",
      "        \n",
      "    faculty_education.append(p_text)\n",
      "#     print faculty_education\n",
      "    return tmp_num\n",
      "\n",
      "# call and run\n",
      "get_letter_sections('http://hs.umt.edu/hs/faculty-list/default.php')\n",
      "\n",
      "for j in letter_section:\n",
      "    get_faculty_info(j)\n",
      "\n",
      "\n",
      "for each_tr in faculty_detail:\n",
      "    info = each_tr.find_all(\"td\")\n",
      "    tmp = 0\n",
      "    for td in info:\n",
      "        tmp = tmp + 1\n",
      "        if (tmp == 1):\n",
      "            # get faculty name\n",
      "            name = td.text\n",
      "            name = unicodedata.normalize('NFKD', name).encode('ascii','ignore')\n",
      "            faculty_full_name.append(name)\n",
      "            \n",
      "            # get faculty detailed link\n",
      "            url = td.find(\"a\", href=True)\n",
      "            personal_url = BASE_PERSONAL_URL + url['href'][2:]\n",
      "            faculty_url.append(personal_url)\n",
      "            \n",
      "        elif (tmp == 2):\n",
      "            dept = td.text\n",
      "            dept = unicodedata.normalize('NFKD', dept).encode('ascii','ignore')\n",
      "            faculty_department.append(dept)\n",
      "\n",
      "\n",
      "\n",
      "# split first and last names\n",
      "for full_name in faculty_full_name:\n",
      "    tmp_name = full_name.split(',')\n",
      "    faculty_last_name.append(tmp_name[0])\n",
      "    faculty_first_name.append(tmp_name[1])\n",
      "\n",
      "# add university\n",
      "for f in range(len(faculty_full_name)):\n",
      "    faculty_university.append(\"University of Montana\")\n",
      "\n",
      "for l in faculty_url:\n",
      "    get_faculty_edu(l)\n",
      "# num = 0\n",
      "# for l in faculty_url:\n",
      "#     num = num + get_faculty_edu(l)\n",
      "# # 283 faculties have PhD education    \n",
      "# print num\n",
      "\n",
      "faculty.append(faculty_first_name)\n",
      "faculty.append(faculty_last_name)\n",
      "faculty.append(faculty_university)\n",
      "faculty.append(faculty_department)\n",
      "faculty.append(faculty_education)\n",
      "\n",
      "with open(\"UniversityOfMontana.csv\", \"wb\") as f:\n",
      "    writer = csv.writer(f) \n",
      "    for row in faculty:\n",
      "        writer.writerow(row)\n",
      "  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 367
    }
   ],
   "metadata": {}
  }
 ]
}