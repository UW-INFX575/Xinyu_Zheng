{
 "metadata": {
  "name": "",
  "signature": "sha256:1b9dc2ba1f456859ea38c4dbf4749affe7fd1b454df19121333466044b5a5a8c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import libraries for processing\n",
      "import nltk\n",
      "import unicodedata\n",
      "import string\n",
      "from nltk import stem\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk import bigrams\n",
      "from nltk import trigrams\n",
      "\n",
      "# initialize stop set, single letters, special characters\n",
      "stopset = set(stopwords.words('english'))\n",
      "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
      "special_char = ['~~']\n",
      "\n",
      "# initialize array to save result, file names, and new files to save result\n",
      "uni_cleaned = [[]]*11\n",
      "file_names = ['6334220.txt','6334221.txt','6334222.txt','6334223.txt','6334224.txt','6334225.txt','6334226.txt','6334227.txt','6334228.txt','6334229.txt']\n",
      "uni_files = ['/Users/XinyuZ/original_files/6334220_uni.txt','/Users/XinyuZ/original_files/6334221_uni.txt','/Users/XinyuZ/original_files/6334222_uni.txt','/Users/XinyuZ/original_files/6334223_uni.txt','/Users/XinyuZ/original_files/6334224_uni.txt','/Users/XinyuZ/original_files/6334225_uni.txt','/Users/XinyuZ/original_files/6334226_uni.txt','/Users/XinyuZ/original_files/6334227_uni.txt','/Users/XinyuZ/original_files/6334228_uni.txt','/Users/XinyuZ/original_files/6334229_uni.txt']\n",
      "number_text = len(file_names)\n",
      "\n",
      "# define a word_clean function for individual files\n",
      "def word_clean_individual(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        # split text file into tokens\n",
      "        tokens=word_tokenize(str(text))    \n",
      "        uni_tokens = [w for w in tokens if not w in stopset]\n",
      "        \n",
      "        # remove stop words, stem words, digits, single letters, and special characters\n",
      "        # and get cleaned unigrams\n",
      "        uni_stemmed = []\n",
      "        stemmer=stem.PorterStemmer()\n",
      "        for w in uni_tokens:\n",
      "            w = w.lower()\n",
      "            tmp = stemmer.stem(w)\n",
      "            tmp = unicodedata.normalize('NFKD', tmp).encode('ascii','ignore')\n",
      "            if tmp not in string.punctuation:\n",
      "                if not tmp.isdigit():\n",
      "                    if tmp not in letters:\n",
      "                        if tmp not in special_char:\n",
      "                            uni_stemmed.append(tmp)\n",
      "                            uni_cleaned[index] = uni_stemmed\n",
      "    return uni_cleaned\n",
      "\n",
      "# call word_clean function for each individual original file and write cleaned results into new files\n",
      "for i in range(len(file_names)):\n",
      "    word_clean_individual(file_names[i], i)\n",
      "    \n",
      "    # write cleaned unigrams with counts to a uni_file\n",
      "    with open(uni_files[i], 'w') as f:\n",
      "        for uni in uni_cleaned[i]:   \n",
      "            f.write(uni + '\\n')\n",
      "        f.close()\n",
      "\n",
      "# define a function to combine the contents of all files into one array\n",
      "uni_tokens = []\n",
      "def word_clean_combined(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        tokens=word_tokenize(str(text)) \n",
      "        for w in tokens:\n",
      "            uni_tokens.append(w)\n",
      "    return uni_tokens\n",
      "\n",
      "# combine all contents of all files into an array of words\n",
      "for i in range(len(file_names)):\n",
      "    word_clean_combined(file_names[i], i)\n",
      "\n",
      "# write the above result into a new file\n",
      "with open('vocab.txt', 'w') as f:\n",
      "    for uni in uni_tokens:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      "\n",
      "# clean the new file\n",
      "j = len(file_names)\n",
      "word_clean_individual('vocab.txt', j)\n",
      "\n",
      "# define a function that generates vocab (unrepeated words) from an array of words\n",
      "vocab = []\n",
      "def generate_vocab(array):\n",
      "    seen = set()\n",
      "    for item in array:\n",
      "        if item not in seen:\n",
      "            seen.add(item)\n",
      "            vocab.append(item)\n",
      "    return True\n",
      "\n",
      "# generate vocab for the combined file\n",
      "generate_vocab(uni_cleaned[j])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.feature_extraction.text import CountVectorizer \n",
      "import os\n",
      "\n",
      "# define a function that creates a term document matrix as pandas dataframe\n",
      "# **kwargs indicates the arguments of CountVectorizer that can be passed\n",
      "def fn_tdm_df(docs, xColNames = None, **kwargs):\n",
      "    #initialize the  vectorizer\n",
      "    vectorizer = CountVectorizer(**kwargs)\n",
      "    x1 = vectorizer.fit_transform(docs)\n",
      "    #create dataFrame\n",
      "    df = pd.DataFrame(x1.toarray().transpose(), index = vectorizer.get_feature_names())\n",
      "    if xColNames is not None:\n",
      "        df.columns = xColNames\n",
      "    return df\n",
      "\n",
      "# provide directory where files (to be used to generate the matrix) are saved\n",
      "DIR = '/Users/XinyuZ/original_files/'\n",
      "\n",
      "# define a function that creates corpus from a dictionary\n",
      "def fn_CorpusFromDIR(xDIR):\n",
      "    Res = dict(docs = [open(os.path.join(xDIR,f)).read() for f in os.listdir(xDIR)],\n",
      "               ColNames = map(lambda x: x[0:], os.listdir(xDIR)))\n",
      "    return Res\n",
      "\n",
      "# call functions above to create a term document matrix as a pandas dataframe\n",
      "d1 = fn_tdm_df(docs = fn_CorpusFromDIR(DIR)['docs'],\n",
      "          xColNames = fn_CorpusFromDIR(DIR)['ColNames'], \n",
      "          stop_words='english', charset_error = 'replace') \n",
      "\n",
      "# arrange the result to certain format for LDA use\n",
      "d1 = d1.drop(['.DS_Store'], axis=1)\n",
      "d1_copy = d1.transpose()\n",
      "print np.array(d1_copy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 0 0 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 2]\n",
        " [0 0 0 ..., 0 0 0]\n",
        " ..., \n",
        " [0 0 0 ..., 0 0 0]\n",
        " [3 5 6 ..., 0 0 0]\n",
        " [0 0 0 ..., 0 0 0]]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/XinyuZ/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:654: DeprecationWarning: The charset_error parameter is deprecated as of version 0.14 and will be removed in 0.16. Use decode_error instead.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import lda\n",
      "\n",
      "# specify parameters for LDA model -  number of topics, number of iterations, number of words per topic)\n",
      "# and fit the model with generated term document matrix\n",
      "model = lda.LDA(n_topics=10, n_iter=1000, random_state=1)\n",
      "model.fit(np.array(d1_copy))\n",
      "\n",
      "topic_word = model.topic_word_  # model.components_ also works\n",
      "n_top_words = 10\n",
      "\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:lda:all zero column in document-term matrix found\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 0: meet main defin fulli loosen end conduct adjust thereof\n",
        "Topic 1: screw sock height thereof foot fulli opposit seat-support person\n",
        "Topic 2: foot sidewal thereof semirigid displac either peripheri rim 40:1\n",
        "Topic 3: either brace cloth method move strip loop nut inclin\n",
        "Topic 4: close rest wall amount main remov rim adhes displac\n",
        "Topic 5: screw remov predetermin torqu within fan effect plate strap\n",
        "Topic 6: part posit main edg upon sheet guid strap fix\n",
        "Topic 7: main brace move top guid screw recit convers ledg\n",
        "Topic 8: 50.80 inhibit maintain main gener adjust fulli rang collect\n",
        "Topic 9: opposit screw rim ride an rubber main meet inhibit\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create toy data sets and write into files\n",
      "# toy datasets: first two docs are related to topic 'apple' and last three docs are related to topic 'books' and 'reading'\n",
      "\n",
      "with open('/Users/XinyuZ/toy_docs/toy_doc_1.txt', 'w') as f:       \n",
      "    f.write(\"Apple is my favorite fruit. It is also very healthy. I usually eat an apple in the morning. It keeps me fresh and energetic.\")\n",
      "    f.close()\n",
      "\n",
      "with open('/Users/XinyuZ/toy_docs/toy_doc_2.txt', 'w') as f:       \n",
      "    f.write(\"I own a farm where I grow a lot of apple trees. I have a lot of work to do at the apple harvest, especially in the morning, but the profit always makes me energetic.\")\n",
      "    f.close()\n",
      "\n",
      "with open('/Users/XinyuZ/toy_docs/toy_doc_3.txt', 'w') as f:       \n",
      "    f.write(\"Peter loves reading and writing notes. He goes to the library every week and writes readers response for every book he reads.\")\n",
      "    f.close()\n",
      "\n",
      "with open('/Users/XinyuZ/toy_docs/toy_doc_4.txt', 'w') as f:       \n",
      "    f.write(\"Henry can read very fast. He can read almost a thousand words per minute. Last week he read ten books borrowed from the library and even remembered almost every word!\")\n",
      "    f.close()\n",
      "    \n",
      "with open('/Users/XinyuZ/toy_docs/toy_doc_5.txt', 'w') as f:       \n",
      "    f.write(\"The new public library attracts tens of thousands of readers every day. They borrow books, read books, or scan books for response writing.\")\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "import unicodedata\n",
      "import string\n",
      "from nltk import stem\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "from nltk import bigrams\n",
      "from nltk import trigrams\n",
      "\n",
      "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
      "uni_cleaned = [[]]*8\n",
      "file_names = ['/Users/XinyuZ/toy_docs/toy_doc_1.txt','/Users/XinyuZ/toy_docs/toy_doc_2.txt','/Users/XinyuZ/toy_docs/toy_doc_3.txt','/Users/XinyuZ/toy_docs/toy_doc_4.txt','/Users/XinyuZ/toy_docs/toy_doc_5.txt']\n",
      "stopset = set(stopwords.words('english'))\n",
      "\n",
      "# define a function to clean words for a file\n",
      "def clean_words(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        # split text file into tokens\n",
      "        tokens=word_tokenize(str(text))    \n",
      "        uni_tokens = [w for w in tokens if not w in stopset]\n",
      "        \n",
      "        # remove stop words, stem words, digits, single letters, and special characters\n",
      "        # and get cleaned unigrams\n",
      "        uni_stemmed = []\n",
      "        stemmer=stem.PorterStemmer()\n",
      "        for w in uni_tokens:\n",
      "            w = w.lower()\n",
      "            tmp = stemmer.stem(w)\n",
      "            tmp = unicodedata.normalize('NFKD', tmp).encode('ascii','ignore')\n",
      "            if tmp not in string.punctuation:\n",
      "                if not tmp.isdigit():\n",
      "                    if tmp not in letters:\n",
      "                        uni_stemmed.append(tmp)\n",
      "                        uni_cleaned[index] = uni_stemmed\n",
      "    return uni_cleaned[index]\n",
      "# uni_cleaned is an array of clean words from each file\n",
      "\n",
      "# clean words for each toy dataset file\n",
      "for i in range(len(file_names)):\n",
      "    print clean_words(file_names[i], i)\n",
      "    print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'appl', 'morn', 'it', 'keep', 'fresh', 'energet']\n",
        "\n",
        "['farm', 'grow', 'lot', 'appl', 'tree', 'lot', 'work', 'appl', 'harvest', 'especi', 'morn', 'profit', 'alway', 'make', 'energet']\n",
        "\n",
        "['peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'write', 'reader', 'respons', 'everi', 'book', 'read']\n",
        "\n",
        "['henri', 'read', 'fast', 'he', 'read', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'week', 'read', 'ten', 'book', 'borrow', 'librari', 'even', 'rememb', 'almost', 'everi', 'word']\n",
        "\n",
        "['the', 'new', 'public', 'librari', 'attract', 'ten', 'thousand', 'reader', 'everi', 'day', 'they', 'borrow', 'book', 'read', 'book', 'scan', 'book', 'respons', 'write']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate documents for the first combined group\n",
      "\n",
      "# initialize arrays to save results\n",
      "uni_tokens_one = []\n",
      "uni_tokens_two = []\n",
      "uni_tokens_corpus = []\n",
      "\n",
      "# specify stopset and different file groups\n",
      "stopset = set(stopwords.words('english'))\n",
      "group_one = ['/Users/XinyuZ/toy_docs/toy_doc_1.txt','/Users/XinyuZ/toy_docs/toy_doc_2.txt']\n",
      "group_two = ['/Users/XinyuZ/toy_docs/toy_doc_3.txt','/Users/XinyuZ/toy_docs/toy_doc_4.txt','/Users/XinyuZ/toy_docs/toy_doc_5.txt']\n",
      "file_names = ['/Users/XinyuZ/toy_docs/toy_doc_1.txt','/Users/XinyuZ/toy_docs/toy_doc_2.txt','/Users/XinyuZ/toy_docs/toy_doc_3.txt','/Users/XinyuZ/toy_docs/toy_doc_4.txt','/Users/XinyuZ/toy_docs/toy_doc_5.txt']\n",
      "\n",
      "# define a function to combine words in group one\n",
      "def combine_words_for_group_one(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        tokens=word_tokenize(str(text)) \n",
      "        for w in tokens:\n",
      "            uni_tokens_one.append(w)\n",
      "    return uni_tokens_one\n",
      "\n",
      "# combine all words in group one\n",
      "for i in range(len(group_one)):\n",
      "    combine_words_for_group_one(group_one[i], i)\n",
      "\n",
      "# write all words in group one into 1 file  called 'group_one.txt'\n",
      "with open('/Users/XinyuZ/group_docs/group_one.txt', 'w') as f:\n",
      "    for uni in uni_tokens_one:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      "\n",
      "# call clean_words function to clean words for group one\n",
      "clean_words('/Users/XinyuZ/group_docs/group_one.txt', 5)\n",
      "\n",
      "# generate documents for the second combined group\n",
      "\n",
      "# define a function to combine words in group two\n",
      "def clean_words_for_group_two(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        tokens=word_tokenize(str(text)) \n",
      "        for w in tokens:\n",
      "            uni_tokens_two.append(w)\n",
      "    return uni_tokens_two\n",
      "\n",
      "# combine all words in group two\n",
      "for i in range(len(group_two)):\n",
      "    clean_words_for_group_two(group_two[i], i)\n",
      "\n",
      "# write all words in group two into 1 file  called 'group_two.txt'\n",
      "with open('/Users/XinyuZ/group_docs/group_two.txt', 'w') as f:\n",
      "    for uni in uni_tokens_two:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      " \n",
      "# call clean_words function to clean words for group two\n",
      "clean_words('/Users/XinyuZ/group_docs/group_two.txt', 6)\n",
      "\n",
      "# generate documents for the whole corpus\n",
      "\n",
      "# define a function to combine words in the whole corpus\n",
      "def clean_words_for_corpus(a_file, index):\n",
      "    with open(a_file, 'r') as text_file:\n",
      "        text = text_file.read()\n",
      "        tokens=word_tokenize(str(text)) \n",
      "        for w in tokens:\n",
      "            uni_tokens_corpus.append(w)\n",
      "    return uni_tokens_corpus\n",
      "\n",
      "# combine all words in whole corpus\n",
      "for i in range(len(file_names)):\n",
      "    clean_words_for_corpus(file_names[i], i)\n",
      "\n",
      "# write all words in corpus into 1 file  called 'corpus.txt'\n",
      "with open('/Users/XinyuZ/group_docs/corpus.txt', 'w') as f:\n",
      "    for uni in uni_tokens_corpus:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      "\n",
      "# call clean_words function to clean words for whole corpus\n",
      "clean_words('/Users/XinyuZ/group_docs/corpus.txt', 7)\n",
      "\n",
      "# make a copy of cleaned words for the five original toy sets, group one, group two, and whole corpus\n",
      "uni_cleaned_copy = uni_cleaned\n",
      "\n",
      "print uni_cleaned_copy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'appl', 'morn', 'it', 'keep', 'fresh', 'energet'], ['farm', 'grow', 'lot', 'appl', 'tree', 'lot', 'work', 'appl', 'harvest', 'especi', 'morn', 'profit', 'alway', 'make', 'energet'], ['peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'write', 'reader', 'respons', 'everi', 'book', 'read'], ['henri', 'read', 'fast', 'he', 'read', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'week', 'read', 'ten', 'book', 'borrow', 'librari', 'even', 'rememb', 'almost', 'everi', 'word'], ['the', 'new', 'public', 'librari', 'attract', 'ten', 'thousand', 'reader', 'everi', 'day', 'they', 'borrow', 'book', 'read', 'book', 'scan', 'book', 'respons', 'write'], ['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'appl', 'morn', 'it', 'keep', 'fresh', 'energet', 'farm', 'grow', 'lot', 'appl', 'tree', 'lot', 'work', 'appl', 'harvest', 'especi', 'morn', 'profit', 'alway', 'make', 'energet'], ['peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'write', 'reader', 'respons', 'everi', 'book', 'read', 'henri', 'read', 'fast', 'he', 'read', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'week', 'read', 'ten', 'book', 'borrow', 'librari', 'even', 'rememb', 'almost', 'everi', 'word', 'the', 'new', 'public', 'librari', 'attract', 'ten', 'thousand', 'reader', 'everi', 'day', 'they', 'borrow', 'book', 'read', 'book', 'scan', 'book', 'respons', 'write'], ['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'appl', 'morn', 'it', 'keep', 'fresh', 'energet', 'farm', 'grow', 'lot', 'appl', 'tree', 'lot', 'work', 'appl', 'harvest', 'especi', 'morn', 'profit', 'alway', 'make', 'energet', 'peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'write', 'reader', 'respons', 'everi', 'book', 'read', 'henri', 'read', 'fast', 'he', 'read', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'week', 'read', 'ten', 'book', 'borrow', 'librari', 'even', 'rememb', 'almost', 'everi', 'word', 'the', 'new', 'public', 'librari', 'attract', 'ten', 'thousand', 'reader', 'everi', 'day', 'they', 'borrow', 'book', 'read', 'book', 'scan', 'book', 'respons', 'write']]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# initialize an array of array to save vocabs of each file\n",
      "result = [[]]*(len(uni_cleaned_copy))\n",
      "\n",
      "# define a function to create vocab for an array\n",
      "def generate_vocab(cleaned_array, index):\n",
      "    seen = set()\n",
      "    result[index] = []\n",
      "    for item in uni_cleaned_copy[index]:\n",
      "        if item not in seen:\n",
      "            seen.add(item)\n",
      "            result[index].append(item)\n",
      "    return result[index]    \n",
      "\n",
      "# generate vocabs for each of the 5 original toy datasets, group one, group two, and whole corpus\n",
      "for i in range(len(uni_cleaned_copy)):\n",
      "    generate_vocab(uni_cleaned_copy[i], i)\n",
      "\n",
      "print result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'morn', 'keep', 'fresh', 'energet'], ['farm', 'grow', 'lot', 'appl', 'tree', 'work', 'harvest', 'especi', 'morn', 'profit', 'alway', 'make', 'energet'], ['peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'reader', 'respons', 'book'], ['henri', 'read', 'fast', 'he', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'week', 'ten', 'book', 'borrow', 'librari', 'even', 'rememb', 'everi'], ['the', 'new', 'public', 'librari', 'attract', 'ten', 'thousand', 'reader', 'everi', 'day', 'they', 'borrow', 'book', 'read', 'scan', 'respons', 'write'], ['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'morn', 'keep', 'fresh', 'energet', 'farm', 'grow', 'lot', 'tree', 'work', 'harvest', 'especi', 'profit', 'alway', 'make'], ['peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'reader', 'respons', 'book', 'henri', 'fast', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'ten', 'borrow', 'even', 'rememb', 'the', 'new', 'public', 'attract', 'day', 'they', 'scan'], ['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'morn', 'keep', 'fresh', 'energet', 'farm', 'grow', 'lot', 'tree', 'work', 'harvest', 'especi', 'profit', 'alway', 'make', 'peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'reader', 'respons', 'book', 'henri', 'fast', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'ten', 'borrow', 'even', 'rememb', 'the', 'new', 'public', 'attract', 'day', 'they', 'scan']]\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# vocab for group one\n",
      "print result[5]\n",
      "print\n",
      "# vocab for group two\n",
      "print result[6]\n",
      "print\n",
      "# vocab for whole corpus\n",
      "print result[7]\n",
      "print"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'morn', 'keep', 'fresh', 'energet', 'farm', 'grow', 'lot', 'tree', 'work', 'harvest', 'especi', 'profit', 'alway', 'make']\n",
        "\n",
        "['peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'reader', 'respons', 'book', 'henri', 'fast', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'ten', 'borrow', 'even', 'rememb', 'the', 'new', 'public', 'attract', 'day', 'they', 'scan']\n",
        "\n",
        "['appl', 'favorit', 'fruit', 'it', 'also', 'healthi', 'usual', 'eat', 'morn', 'keep', 'fresh', 'energet', 'farm', 'grow', 'lot', 'tree', 'work', 'harvest', 'especi', 'profit', 'alway', 'make', 'peter', 'love', 'read', 'write', 'note', 'he', 'goe', 'librari', 'everi', 'week', 'reader', 'respons', 'book', 'henri', 'fast', 'almost', 'thousand', 'word', 'per', 'minut', 'last', 'ten', 'borrow', 'even', 'rememb', 'the', 'new', 'public', 'attract', 'day', 'they', 'scan']\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# write group_one's vocab into group_one_vocab.txt\n",
      "with open('/Users/XinyuZ/vocabs/group_one_vocab.txt', 'w') as f:\n",
      "    for uni in result[5]:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      "\n",
      "# write group_two's vocab into group_two_vocab.txt\n",
      "with open('/Users/XinyuZ/vocabs/group_two_vocab.txt', 'w') as f:\n",
      "    for uni in result[6]:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()\n",
      "    \n",
      "# write corpus' vocab into corpus_vocab.txt\n",
      "with open('/Users/XinyuZ/vocabs/corpus_vocab.txt', 'w') as f:\n",
      "    for uni in result[7]:        \n",
      "        f.write(uni + \" \")\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "\n",
      "# initialize arrays to save results\n",
      "prob = [[]]*len(uni_cleaned_copy)\n",
      "wordfreq = [[]]*len(uni_cleaned_copy)\n",
      "total_sum = [0]*len(uni_cleaned_copy)\n",
      "freq = [[]]*len(uni_cleaned_copy)\n",
      "word = [[]]*len(uni_cleaned_copy)\n",
      "\n",
      "# count word frequencies for each toy datasets, group one, group two, and whole corpus\n",
      "for j in range(len(uni_cleaned_copy)):\n",
      "    wordfreq[j] = Counter(uni_cleaned_copy[j]).most_common()\n",
      "\n",
      "# count word occurrance probabilities for each toy datasets, group one, group two, and whole corpus\n",
      "for i in range(len(wordfreq)):\n",
      "    freq[i] = []\n",
      "    word[i] = []\n",
      "    for k in wordfreq[i]:\n",
      "        freq[i].append(k[1])\n",
      "        word[i].append(k[0])\n",
      "\n",
      "    total_sum[i] = 0\n",
      "    for f in freq[i]:\n",
      "        total_sum[i] += f\n",
      "    \n",
      "    prob[i] = []\n",
      "    for p in freq[i]:\n",
      "        prob[i].append(round(p*1.0/total_sum[i],5))\n",
      "   \n",
      "    print \"Pairs\\n\" + str(zip(word[i], freq[i]))\n",
      "    print \"Probabilities\\n\" + str(zip(word[i], prob[i]))\n",
      "    print\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pairs\n",
        "[('it', 2), ('appl', 2), ('energet', 1), ('favorit', 1), ('fruit', 1), ('keep', 1), ('also', 1), ('morn', 1), ('healthi', 1), ('fresh', 1), ('eat', 1), ('usual', 1)]\n",
        "Probabilities\n",
        "[('it', 0.14286), ('appl', 0.14286), ('energet', 0.07143), ('favorit', 0.07143), ('fruit', 0.07143), ('keep', 0.07143), ('also', 0.07143), ('morn', 0.07143), ('healthi', 0.07143), ('fresh', 0.07143), ('eat', 0.07143), ('usual', 0.07143)]\n",
        "\n",
        "Pairs\n",
        "[('appl', 2), ('lot', 2), ('energet', 1), ('especi', 1), ('farm', 1), ('make', 1), ('work', 1), ('tree', 1), ('alway', 1), ('profit', 1), ('morn', 1), ('harvest', 1), ('grow', 1)]\n",
        "Probabilities\n",
        "[('appl', 0.13333), ('lot', 0.13333), ('energet', 0.06667), ('especi', 0.06667), ('farm', 0.06667), ('make', 0.06667), ('work', 0.06667), ('tree', 0.06667), ('alway', 0.06667), ('profit', 0.06667), ('morn', 0.06667), ('harvest', 0.06667), ('grow', 0.06667)]\n",
        "\n",
        "Pairs\n",
        "[('read', 2), ('write', 2), ('everi', 2), ('week', 1), ('note', 1), ('love', 1), ('respons', 1), ('goe', 1), ('book', 1), ('reader', 1), ('peter', 1), ('librari', 1), ('he', 1)]\n",
        "Probabilities\n",
        "[('read', 0.125), ('write', 0.125), ('everi', 0.125), ('week', 0.0625), ('note', 0.0625), ('love', 0.0625), ('respons', 0.0625), ('goe', 0.0625), ('book', 0.0625), ('reader', 0.0625), ('peter', 0.0625), ('librari', 0.0625), ('he', 0.0625)]\n",
        "\n",
        "Pairs\n",
        "[('read', 3), ('word', 2), ('almost', 2), ('week', 1), ('even', 1), ('everi', 1), ('ten', 1), ('henri', 1), ('thousand', 1), ('borrow', 1), ('fast', 1), ('book', 1), ('per', 1), ('rememb', 1), ('last', 1), ('minut', 1), ('librari', 1), ('he', 1)]\n",
        "Probabilities\n",
        "[('read', 0.13636), ('word', 0.09091), ('almost', 0.09091), ('week', 0.04545), ('even', 0.04545), ('everi', 0.04545), ('ten', 0.04545), ('henri', 0.04545), ('thousand', 0.04545), ('borrow', 0.04545), ('fast', 0.04545), ('book', 0.04545), ('per', 0.04545), ('rememb', 0.04545), ('last', 0.04545), ('minut', 0.04545), ('librari', 0.04545), ('he', 0.04545)]\n",
        "\n",
        "Pairs\n",
        "[('book', 3), ('everi', 1), ('ten', 1), ('scan', 1), ('read', 1), ('thousand', 1), ('respons', 1), ('borrow', 1), ('day', 1), ('write', 1), ('they', 1), ('reader', 1), ('new', 1), ('the', 1), ('attract', 1), ('public', 1), ('librari', 1)]\n",
        "Probabilities\n",
        "[('book', 0.15789), ('everi', 0.05263), ('ten', 0.05263), ('scan', 0.05263), ('read', 0.05263), ('thousand', 0.05263), ('respons', 0.05263), ('borrow', 0.05263), ('day', 0.05263), ('write', 0.05263), ('they', 0.05263), ('reader', 0.05263), ('new', 0.05263), ('the', 0.05263), ('attract', 0.05263), ('public', 0.05263), ('librari', 0.05263)]\n",
        "\n",
        "Pairs\n",
        "[('appl', 4), ('it', 2), ('morn', 2), ('lot', 2), ('energet', 2), ('favorit', 1), ('also', 1), ('profit', 1), ('make', 1), ('eat', 1), ('alway', 1), ('especi', 1), ('farm', 1), ('fruit', 1), ('harvest', 1), ('grow', 1), ('work', 1), ('tree', 1), ('keep', 1), ('healthi', 1), ('fresh', 1), ('usual', 1)]\n",
        "Probabilities\n",
        "[('appl', 0.13793), ('it', 0.06897), ('morn', 0.06897), ('lot', 0.06897), ('energet', 0.06897), ('favorit', 0.03448), ('also', 0.03448), ('profit', 0.03448), ('make', 0.03448), ('eat', 0.03448), ('alway', 0.03448), ('especi', 0.03448), ('farm', 0.03448), ('fruit', 0.03448), ('harvest', 0.03448), ('grow', 0.03448), ('work', 0.03448), ('tree', 0.03448), ('keep', 0.03448), ('healthi', 0.03448), ('fresh', 0.03448), ('usual', 0.03448)]\n",
        "\n",
        "Pairs\n",
        "[('read', 6), ('book', 5), ('everi', 4), ('librari', 3), ('write', 3), ('ten', 2), ('almost', 2), ('respons', 2), ('reader', 2), ('week', 2), ('thousand', 2), ('word', 2), ('he', 2), ('borrow', 2), ('love', 1), ('scan', 1), ('per', 1), ('peter', 1), ('attract', 1), ('even', 1), ('goe', 1), ('fast', 1), ('day', 1), ('note', 1), ('rememb', 1), ('new', 1), ('public', 1), ('they', 1), ('the', 1), ('last', 1), ('henri', 1), ('minut', 1)]\n",
        "Probabilities\n",
        "[('read', 0.10526), ('book', 0.08772), ('everi', 0.07018), ('librari', 0.05263), ('write', 0.05263), ('ten', 0.03509), ('almost', 0.03509), ('respons', 0.03509), ('reader', 0.03509), ('week', 0.03509), ('thousand', 0.03509), ('word', 0.03509), ('he', 0.03509), ('borrow', 0.03509), ('love', 0.01754), ('scan', 0.01754), ('per', 0.01754), ('peter', 0.01754), ('attract', 0.01754), ('even', 0.01754), ('goe', 0.01754), ('fast', 0.01754), ('day', 0.01754), ('note', 0.01754), ('rememb', 0.01754), ('new', 0.01754), ('public', 0.01754), ('they', 0.01754), ('the', 0.01754), ('last', 0.01754), ('henri', 0.01754), ('minut', 0.01754)]\n",
        "\n",
        "Pairs\n",
        "[('read', 6), ('book', 5), ('appl', 4), ('everi', 4), ('librari', 3), ('write', 3), ('ten', 2), ('almost', 2), ('it', 2), ('respons', 2), ('morn', 2), ('lot', 2), ('reader', 2), ('week', 2), ('energet', 2), ('thousand', 2), ('he', 2), ('word', 2), ('borrow', 2), ('the', 1), ('love', 1), ('scan', 1), ('favorit', 1), ('per', 1), ('peter', 1), ('attract', 1), ('even', 1), ('note', 1), ('also', 1), ('profit', 1), ('make', 1), ('fast', 1), ('eat', 1), ('alway', 1), ('new', 1), ('public', 1), ('day', 1), ('rememb', 1), ('especi', 1), ('farm', 1), ('fruit', 1), ('they', 1), ('last', 1), ('harvest', 1), ('grow', 1), ('henri', 1), ('work', 1), ('tree', 1), ('keep', 1), ('goe', 1), ('healthi', 1), ('fresh', 1), ('minut', 1), ('usual', 1)]\n",
        "Probabilities\n",
        "[('read', 0.06977), ('book', 0.05814), ('appl', 0.04651), ('everi', 0.04651), ('librari', 0.03488), ('write', 0.03488), ('ten', 0.02326), ('almost', 0.02326), ('it', 0.02326), ('respons', 0.02326), ('morn', 0.02326), ('lot', 0.02326), ('reader', 0.02326), ('week', 0.02326), ('energet', 0.02326), ('thousand', 0.02326), ('he', 0.02326), ('word', 0.02326), ('borrow', 0.02326), ('the', 0.01163), ('love', 0.01163), ('scan', 0.01163), ('favorit', 0.01163), ('per', 0.01163), ('peter', 0.01163), ('attract', 0.01163), ('even', 0.01163), ('note', 0.01163), ('also', 0.01163), ('profit', 0.01163), ('make', 0.01163), ('fast', 0.01163), ('eat', 0.01163), ('alway', 0.01163), ('new', 0.01163), ('public', 0.01163), ('day', 0.01163), ('rememb', 0.01163), ('especi', 0.01163), ('farm', 0.01163), ('fruit', 0.01163), ('they', 0.01163), ('last', 0.01163), ('harvest', 0.01163), ('grow', 0.01163), ('henri', 0.01163), ('work', 0.01163), ('tree', 0.01163), ('keep', 0.01163), ('goe', 0.01163), ('healthi', 0.01163), ('fresh', 0.01163), ('minut', 0.01163), ('usual', 0.01163)]\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# comments:\n",
      "# word[5] has all unrepeated words in group 1\n",
      "# prob[5] has all probabilities of corresponding words in word[5]\n",
      "\n",
      "# word[6] has all unrepeated words in group 2\n",
      "# prob[6] has all probabilities of corresponding words in word[6]\n",
      "\n",
      "# word[7] has all unrepeated words in group 3\n",
      "# prob[7] has all probabilities of corresponding words in word[7]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "# calculate H(xi) for group one\n",
      "log_sum_group1 = 0\n",
      "for i in prob[5]:\n",
      "    tmp_log = math.log(i,2)\n",
      "    log_sum_group1 += i * tmp_log\n",
      "H_group1 = 0 - log_sum_group1\n",
      "print H_group1\n",
      "\n",
      "# calculate H(xi) for group two\n",
      "log_sum_group2 = 0\n",
      "for j in prob[6]:\n",
      "    tmp_log = math.log(j,2)\n",
      "    log_sum_group2 += j * tmp_log\n",
      "H_group2 = 0 - log_sum_group2\n",
      "print H_group2\n",
      "\n",
      "# calculate H(xi) for whole corpus\n",
      "log_sum_corpus = 0\n",
      "for k in prob[7]:\n",
      "    tmp_log = math.log(k,2)\n",
      "    log_sum_corpus += k * tmp_log\n",
      "H_corpus = 0 - log_sum_corpus\n",
      "print H_corpus"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4.306138533\n",
        "4.73389465394\n",
        "5.5125260486\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate Q12\n",
      "import math\n",
      "\n",
      "tmp_Q12 = 0\n",
      "for i in range(len(word[5])):\n",
      "    wi = word[5][i]\n",
      "    \n",
      "    if wi in word[6]:\n",
      "        ind = word[6].index(wi)\n",
      "        pj = prob[6][ind]\n",
      "        pi = prob[5][i]\n",
      "    else:\n",
      "        alpha = 0.01\n",
      "        ind = word[7].index(wi)\n",
      "        sj = prob[7][ind]\n",
      "        pj = alpha * sj\n",
      "        pi = (1-alpha)*prob[5][i]+alpha*sj\n",
      "    \n",
      "    tmp_Q12 += pi * math.log(pj,2)\n",
      "Q_12 = 0 - tmp_Q12\n",
      "print 'Q_12:', Q_12\n",
      "\n",
      "\n",
      "# calculate Q21\n",
      "tmp_Q21 = 0\n",
      "for i in range(len(word[6])):\n",
      "    wi = word[6][i]\n",
      "    \n",
      "    if wi in word[5]:\n",
      "        ind = word[5].index(wi)\n",
      "        pj = prob[5][ind]\n",
      "        pi = prob[6][i]\n",
      "    else:\n",
      "        alpha = 0.01\n",
      "        ind = word[7].index(wi)\n",
      "        sj = prob[7][ind]\n",
      "        pj = alpha * sj\n",
      "        pi = (1-alpha)*prob[6][i]+alpha*sj\n",
      "    \n",
      "    tmp_Q21 += pi * math.log(pj,2)\n",
      "Q_21 = 0 - tmp_Q21\n",
      "print 'Q_21:', Q_21"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Q_12: 12.4348132056\n",
        "Q_21: 11.9301882073\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate E12\n",
      "E_12 = H_group1 / Q_12\n",
      "print 'E_12:', E_12\n",
      "\n",
      "# calculate E21\n",
      "E_21 = H_group2 / Q_21\n",
      "print 'E_21:', E_21\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "E_12: 0.346297001957\n",
        "E_21: 0.396799662477\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate C12\n",
      "C_12 = 1 - E_12\n",
      "print 'C_12:', C_12\n",
      "\n",
      "# calculate C21\n",
      "C_21 = 1 - E_21\n",
      "print 'C_21:', C_21"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_12: 0.653702998043\n",
        "C_21: 0.603200337523\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate C1\n",
      "C_1 = C_12\n",
      "print 'C_1:', C_1\n",
      "\n",
      "# calculate C2\n",
      "C_2 = C_21\n",
      "print 'C_2:', C_2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C_1: 0.653702998043\n",
        "C_2: 0.603200337523\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# run LDA on toy dataset\n",
      "\n",
      "import pandas as pd\n",
      "from sklearn.feature_extraction.text import CountVectorizer \n",
      "\n",
      "# define a function that creates a term document matrix as pandas dataframe\n",
      "# **kwargs indicates the arguments of CountVectorizer that can be passed\n",
      "def fn_tdm_df(docs, xColNames = None, **kwargs):\n",
      "\n",
      "    #initialize the  vectorizer\n",
      "    vectorizer = CountVectorizer(**kwargs)\n",
      "    x1 = vectorizer.fit_transform(docs)\n",
      "    \n",
      "    #create dataFrame\n",
      "    df = pd.DataFrame(x1.toarray().transpose(), index = vectorizer.get_feature_names())\n",
      "    if xColNames is not None:\n",
      "        df.columns = xColNames\n",
      "\n",
      "    return df\n",
      "\n",
      "# provide directory where files (to be used to generate the matrix) are saved\n",
      "DIR = '/Users/XinyuZ/toy_docs/'\n",
      "\n",
      "# define a function that creates corpus from a dictionary\n",
      "def fn_CorpusFromDIR(xDIR):\n",
      "    ''' functions to create corpus from a Directories\n",
      "    Input: Directory\n",
      "    Output: A dictionary with \n",
      "             Names of files ['ColNames']\n",
      "             the text in corpus ['docs']'''\n",
      "    import os\n",
      "    Res = dict(docs = [open(os.path.join(xDIR,f)).read() for f in os.listdir(xDIR)],\n",
      "               ColNames = map(lambda x: x[0:], os.listdir(xDIR)))\n",
      "    return Res\n",
      "\n",
      "# call functions above to create a term document matrix as a pandas dataframe\n",
      "df_toy = fn_tdm_df(docs = fn_CorpusFromDIR(DIR)['docs'],\n",
      "          xColNames = fn_CorpusFromDIR(DIR)['ColNames'], \n",
      "          stop_words='english', charset_error = 'replace') \n",
      "\n",
      "# arrange the result to certain format for LDA use\n",
      "df_toy_copy = df_toy.transpose()\n",
      "print np.array(df_toy_copy)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[2 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
        "  0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
        " [2 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 2 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
        "  0 0 0 0 0 1 0 0 0 0 1 0 0]\n",
        " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 0\n",
        "  1 0 0 0 0 0 0 1 0 0 0 1 1]\n",
        " [0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 3 0 0 0 1\n",
        "  0 0 0 1 0 0 0 1 1 1 0 0 0]\n",
        " [0 1 0 3 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0\n",
        "  1 1 1 0 1 0 0 0 0 0 0 0 1]]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/XinyuZ/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.py:654: DeprecationWarning: The charset_error parameter is deprecated as of version 0.14 and will be removed in 0.16. Use decode_error instead.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import lda\n",
      "\n",
      "# specify parameters for LDA model -  number of topics, number of iterations, number of words per topic)\n",
      "# and fit the model with generated term document matrix\n",
      "model = lda.LDA(n_topics=2, n_iter=1000, random_state=1)\n",
      "model.fit(np.array(df_toy_copy))\n",
      "\n",
      "topic_word = model.topic_word_  # model.components_ also works\n",
      "n_top_words = 15\n",
      "\n",
      "for i, topic_dist in enumerate(topic_word):\n",
      "    topic_words = np.array(result[7])[np.argsort(topic_dist)][:-n_top_words:-1]\n",
      "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Topic 0: reader it make respons almost public borrow fruit week energet profit favorit love new\n",
        "Topic 1: appl morn peter note everi eat keep fresh farm grow lot work harvest especi\n"
       ]
      }
     ],
     "prompt_number": 19
    }
   ],
   "metadata": {}
  }
 ]
}